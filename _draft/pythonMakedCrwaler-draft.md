---
layout: post
title: "[Python] 크롤링으로 업무 자동화하기 - (1)개요"
date: 2019-11-30 06:00:00
category: python
tag: 
- python
comments: true
---

협업팀에서 API 서비스 명세서 정보가 변경되었는데, 내용이 전달되지 않아 문제가 발생하는 일이 있었다. 긴급하게 수정하여 문제를 해결했지만, 사람이하는 일이기 때문에 언젠가는 또 발생할 수 있는 일이였다. 중요한 문제였기 때문에 나는 수 많은 명세서의 변경을 확인하여 결과를 알려주는 크롤러를 만들었다.

### 고통
현재 필자가 속한 CODEF에서 서비스중인 데이터 중계 API는 스크래핑 엔진을 관리하는 팀과 협업으로 개발&운영 되고 있다. 그리고 이 두 팀간의 소통은 매우 중요하다. 대표적으로 서비스 명세서가 있다. 근데 이 서비스 명세서의 변경된 내용이 전달되지 않아 API에 문제가 발생했다.

고객사에서 급하게 전화가 왔다. 고객사에서 사용중인 API 모듈이 지속적으로 에러를 발생시키고 있던 것이다. 때마침 팀내에서 관리중인 API 상태관리 스케줄러도 API에 문제가 있음을 알려주고 있었다.

긴급하게 로그를 살펴보았다. 고객사가 요청한 파라미터 값은 우리가 기대한 정상적인 값이였다. 하지만 스크래핑 대상 기관에서는 정상적인 데이터 값을 주지않고 있었다. 이상함을 감지하고 해당 API의 서비스 명세서를 확인해보았다. 확인해보니 필수 파라미터 값이 변경되었다는 내용을 확인할 수 있었다.

문제를 인지하고 신속하게 개선하여 긴급반영을 통해 문제를 해결할 수 있었지만, 이런일이 발생한 것에대한 근본적인 문제를 해결할 일이 남아있었다.


### 문제를 해결해보자
협업팀에서 관리하는 API 서비스 명세서의 수는 대략 50개 가까이된다. 그리고 각 명세서 파일의 첫번째 시트에 명세서의 변경된 내용을 로그로 남긴다.

근데 이 변경된 내용이 우리팀에게도 매우 중요하다. 어떤건 API 서비스에 직접적인 문제를 일으킬 수 있을 정도로 크리티컬할 수 있다. 그래서 항상 명세서 정보의 변경을 인지하고 있어야 한다.

매일 변경을 확인하기 위해 할 수 있는 방법은 2가지가 있다.

- 변경 즉시 협업팀이 우리팀에게 내용을 전달한다.
- 매일 특정 시간마다 파일들을 다운 받아 이전 버전과 비교한다.

말은 참 간단하다. 하지만 이 방법은 현실적으로 문제가 있다.

- ~~변경 즉시 협업팀이 우리팀에게 내용을 전달한다.~~ 사람은 실수할 수 있고, 실제로 문제가 발생했다.
- ~~매일 특정 시간마다 파일들을 다운 받아 이전 버전과 비교한다.~~ 한번 할때마다 1~2시간 정도 소요될 수 있는 업무다. 오전, 오후 두번한다고 하면 그건 꽤 큰 리소스 낭비다. (그리고 누가 이걸하고 싶겠나)

현실적인 어려움이 있는 이 문제를 해결하기 위해 1차적으로 협업팀에서 메일 전송을 해주기로 했다. 그런데 실제 중요한 변경만 공유됐고 사소한 변화까지는 공유되지 못하고 있었다.

하지만 나는 사소한 변화도 우리팀이 인지해야 된다고 생각했고, 가능한한 꼼꼼하게 조치를 취해야 된다고 생각했다. 그래서 나는 개발자답게 컴퓨터에게 업무를 지시하기로 결정했다. '매일 오전, 오후 두번 모든 명세서를 비교해서 우리에게 변경사항을 알려줘'라고 말이다.


### 기대효과
컴퓨터가 이 업무를 대신했을때 기대효과는 이러하다.

- 우리가 업무를 잊고있어도 컴퓨터는 문제가 발생하지 않는 이상 잊어버리지 않는다.
- 문제가 발생해도 문제가 발생했다고 알려준다.
- 사람이 1~2시간이라는 시간이 필요할때, 컴퓨터는 1분안에 해결할 수 있다.
- 정말 귀찮고 하기 싫은 일을 군말없이 한다.
- 팀원 모두는 더 생산적인 일에 집중할 수 있다.


### 필요한 프로세스
명세서가 존재하는 페이지에 접근하려면 먼저 로그인을 해야한다.

![crawlerProcess1](/assets/images/post/crawlerProcess1.png){: width="100%"}*\<로그인 페이지\>*

그리고 서비스 명세서를 우클릭해서 다운로드한다.

![crawlerProcess2](/assets/images/post/crawlerProcess2.png){: width="100%"}*\<명세서 다운로드\>*

이렇게 간단하다. 로그인하고 다운로드하면 끝이다. 그리고 다운로드한 디렉토리는 zip파일로 다운로드되고 이를 풀어주면 아래와 같다.

![crawlerProcess3](/assets/images/post/crawlerProcess3.png){: width="100%"}*\<명세서 다운로드 결과\>*

그리고 명세서 내용은 대략 아래와 같이 구성되어 있다. 앞에서도 언급했듯이 이런 파일이 50개 가까이되고 데이터가 많은 파일도 있다.

![crawlerProcess4](/assets/images/post/crawlerProcess4.png){: width="100%"}*\<명세서 내용 예시\>*

이 데이터의 수정, 삭제, 추가된 것을 파악하면 된다.

그리고 이 파악된 정보는 현재 팀내에서 사용중인 **슬랙**으로 전송해주면 된다.

![crawlerProcess5](/assets/images/post/crawlerProcess5.png){: width="100%"}*\<슬랙에 결과 전송\>*

이렇게 내용을 전송하고 다음 비교때 사용하기 위해 현재 받은 파일을 백업하면 마무리된다.

간단하게 정리하면 아래와 같다.

1. 페이지 로그인
2. 서비스 명세서 디렉토리 다운로드
3. 다운로드된 zip파일 풀기
4. 파일 비교한 결과 슬랙으로 전송
5. 다음 파일 비교를 위해 현재 버전 백업



### 사용할 기술을 정하자
#### 파이썬
파이썬은 생산성이 좋고 라이브러리가 매우 훌륭하여 자주 사용하는 프로그래밍 언어다. 특히 높은 생산성과 예쁜 코드 때문에 개인적으로 좋아하는 언어이기도 하다.

앞에서 언급한 프로세스를 자동화하는 프로그램을 만드는데 파이썬이 적합하다고 생각했다. 이유는 아래와 같다.

1. 빠르게 만들 수 있다.
2. 크롤링할 때 selenium과 BeautifulSoup를 같이 사용하면 크롤링한 데이터를 핸들링하기 쉽다.
3. 엑셀을 다룰때 openpyxl이나, pandas같은 라이브러리로 엑셀 데이터를 쉽게 다룰 수 있다.
4. 언급한 라이브러리들의 레퍼런스가 많다.

#### Selenium
파이썬을 사용하여 크롤링 기술을 공부할때 처음으로 접하는 라이브러리는 requests일 것이다. 하지만 requests는 자바스크립트를 동작시킬 수 없고, 이런 이유로 동적 렌더링 페이지의 데이터를 가져오는데 어려움이 있다.

내가 크롤링하려는 대상 페이지는 동적 렌더링 페이지였고, 이를 크롤링하기 위해서 webdriver를 사용해야 한다. 그리고 Selenium 라이브러리가 webdriver를 사용하는데 유용하기 때문에 선택했다.

#### pandas
판다스는 데이터와 관련된 분야에서 많이 사용되는 라이브러리이다. csv, xlsx 파일들을 다룰 때도 유용하다. 데이터를 비교하는데 유용하다고 판단하여 사용했다.

#### slacker
팀내에서 협업툴로 슬랙을 사용하고 있다. 팀내 채팅방으로 메세지를 전송하기 위해선 slacker를 사용해야한다.

